{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee477a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class data_loader:\n",
    "    \n",
    "    def __init__(self, dataset_path, vis=False):\n",
    "        \"\"\"\n",
    "        Initialize the data_loader class\n",
    "        Args:\n",
    "            dataset_path(str): the dataset object address\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dataset_path = dataset_path\n",
    "        self.is_vis = vis\n",
    "        \n",
    "        # The list that use to translate the label to defects type\n",
    "        self.label_keys = [\"Center, \", \"Donut, \", \"Edge_Loc, \", \"Edge_Ring, \", \n",
    "                           \"Loc, \", \"Near_Full, \", \"Scratch, \", \"Random, \"]\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data to the vaiable \"self.train\" and \"self.label\"  \n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = np.load(self.dataset_path)\n",
    "        self.train = self.data[\"arr_0\"]\n",
    "        self.label = self.data[\"arr_1\"]\n",
    "        \n",
    "        print(\"MixedWM38:\", np.shape(self.label)[0], 'wafer loaded')\n",
    "        \n",
    "    def prep_data(self):\n",
    "        \"\"\"\n",
    "        Do train_test_split and format the data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.train, \n",
    "                                                                                self.label, \n",
    "                                                                                test_size = 0.2,\n",
    "                                                                                random_state= 42)\n",
    "        \n",
    "        \n",
    "        self.x_train = np.expand_dims(self.x_train, axis=1).astype(np.float32)\n",
    "        self.x_test = np.expand_dims(self.x_test, axis=1).astype(np.float32)\n",
    "        \n",
    "        self.x_train = torch.from_numpy(self.x_train)\n",
    "        self.x_test = torch.from_numpy(self.x_test)\n",
    "\n",
    "        self.y_train = torch.from_numpy(self.y_train).long()\n",
    "        self.y_test = torch.from_numpy(self.y_test).long()\n",
    "\n",
    "    \n",
    "    def read_label(self, label, defect_types =''):\n",
    "        \"\"\"\n",
    "        Translate the label into defect_types\n",
    "        Args:\n",
    "            label(list): the label that indicate type of  defect, \n",
    "                         for instance [0 1 0 1 0 0 0 1]\n",
    "        return::\n",
    "            defect_types(str): the string that indicate defect type\n",
    "        \"\"\"\n",
    "       \n",
    "        if np.sum(label) == 0:\n",
    "            defect_types = 'Normal wafer'\n",
    "        \n",
    "        else:\n",
    "            for digit in range(np.shape(label)[0]):\n",
    "\n",
    "                if label[digit] == 1:\n",
    "\n",
    "                    defect_types = defect_types + self.label_keys[digit]  \n",
    "        \n",
    "        return defect_types\n",
    "    \n",
    "    def see_wafer(self, wafer_num):\n",
    "        \"\"\"\n",
    "        See the defect pattern in idicated by wafer number\n",
    "        Args:\n",
    "            wafer_num(float): the index of loaded data in self.train\n",
    "\n",
    "        return:\n",
    "            plot(oject): the plot image\n",
    "        \"\"\"\n",
    "        \n",
    "        defect_types = self.read_label(self.label[wafer_num])\n",
    "        \n",
    "        plt.title(\"wafer #\"+str(wafer_num))\n",
    "        plt.imshow(self.train[wafer_num])\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Defect types=\", defect_types)\n",
    "        print(\"Labeled as:\", self.label[wafer_num])\n",
    "        \n",
    "    def get_data(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Get the loaded data \n",
    "\n",
    "        return:\n",
    "            x_train(array): image of defect pattern for model training\n",
    "            x_test(array): image of defect pattern for model validation\n",
    "            y_train(tensor): label of defect pattern for model training\n",
    "            y_test(tensor): label of defect pattern for model validation\n",
    "        \"\"\"\n",
    "            \n",
    "        self.load_data()\n",
    "        self.prep_data()\n",
    "        \n",
    "        return self.x_train, self.x_test, self.y_train, self.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c3f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixedWM38: 38015 wafer loaded\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r'C://Users/isang/OneDrive/Desktop/WM/data/Wafer_Map_Datasets.npz'\n",
    "\n",
    "A1 = data_loader(dataset_path, vis= True)\n",
    "x_train, x_test, y_train, y_test = A1.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d06882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Transformer 블록 정의 ---\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_units, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_units[0]),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_units[0], embed_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, E)\n",
    "        y = self.norm1(x)\n",
    "        attn_out, _ = self.attn(y, y, y)\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        y = self.norm2(x)\n",
    "        x = x + self.mlp(y)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac655c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- MLP 헤드 정의 ---\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_units, num_classes, dropout_rate):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for units in hidden_units:\n",
    "            layers += [nn.Linear(in_features, units), nn.GELU(), nn.Dropout(dropout_rate)]\n",
    "            in_features = units\n",
    "        layers.append(nn.Linear(in_features, num_classes))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d41993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# --- Vision Transformer 모델 정의 ---\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        patch_size: int,\n",
    "        in_channels: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        embed_dim: int,\n",
    "        transformer_units: list,\n",
    "        mlp_head_units: list,\n",
    "        num_classes: int,\n",
    "        dropout_rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_dim = patch_size * patch_size * in_channels\n",
    "\n",
    "        # 패치 분할용\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        # 패치 → 임베딩\n",
    "        self.patch_proj = nn.Linear(self.patch_dim, embed_dim)\n",
    "        # 위치 임베딩\n",
    "        self.position_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
    "\n",
    "        # Transformer 블록 여러 개 쌓기\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, transformer_units, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # 분류를 위한 MLP 헤드\n",
    "        self.mlp_head = MLPHead(embed_dim * self.num_patches, mlp_head_units, num_classes, dropout_rate)\n",
    "\n",
    "    def get_patches(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        images: (B, C, H, W)\n",
    "        return: (B, num_patches, patch_dim)\n",
    "        \"\"\"\n",
    "        B = images.size(0)\n",
    "        patches = self.unfold(images)               # (B, patch_dim, num_patches)\n",
    "        patches = patches.transpose(1, 2)           # (B, num_patches, patch_dim)\n",
    "        return patches\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        return: logits (B, num_classes)\n",
    "        \"\"\"\n",
    "        # 1) 패치 분할 & 투영\n",
    "        patches = self.get_patches(x)               # (B, N, patch_dim)\n",
    "        x = self.patch_proj(patches)               # (B, N, embed_dim)\n",
    "        x = x + self.position_embed                # 위치 임베딩 더하기\n",
    "\n",
    "        # 2) Transformer 블록\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)                           # (B, N, embed_dim)\n",
    "\n",
    "        # 3) 펼쳐서 MLP 헤드로 분류\n",
    "        x = x.flatten(1)                           # (B, N * embed_dim)\n",
    "        logits = self.mlp_head(x)                  # (B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a05000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "\n",
    "# --- 전체 파이프라인 클래스 ---\n",
    "class VisTransformer:\n",
    "    def __init__(self, x_train, x_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_train (Tensor): (N, C, H, W) 형태의 Train Image\n",
    "            x_test  (Tensor): (M, C, H, W) 형태의 Test Image\n",
    "            y_train (Tensor): (N, num_classes) 형태의 원-핫 라벨\n",
    "            y_test  (Tensor): (M, num_classes) 형태의 원-핫 라벨\n",
    "        \"\"\"\n",
    "        # 데이터 저장\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "        # Input size, Patch 관련 파라미터 계산\n",
    "        _, c, H, W = x_train.shape\n",
    "        assert H == W, \"이미지는 정사각형이어야 합니다.\"\n",
    "        self.image_size = H\n",
    "        self.input_shape = (H, W, c)\n",
    "\n",
    "        self.label_size = y_test.size(1)\n",
    "        self.patch_size = 13\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.projection_dim = 96\n",
    "        self.num_heads = 4\n",
    "        self.transformer_units = [self.projection_dim * 2, self.projection_dim]\n",
    "        self.transformer_layers = 16\n",
    "        self.mlp_head_units = [2048, 1024]\n",
    "        self.dropout_rate = 0.1\n",
    "\n",
    "        # Model Instance 생성  \n",
    "        self.model = VisionTransformer(\n",
    "            image_size=self.image_size,\n",
    "            patch_size=self.patch_size,\n",
    "            in_channels=c,\n",
    "            num_layers=self.transformer_layers,\n",
    "            num_heads=self.num_heads,\n",
    "            embed_dim=self.projection_dim,\n",
    "            transformer_units=self.transformer_units,\n",
    "            mlp_head_units=self.mlp_head_units,\n",
    "            num_classes=self.label_size,\n",
    "            dropout_rate=self.dropout_rate\n",
    "        )\n",
    "\n",
    "    def augmentation(self):\n",
    "        \"\"\"\n",
    "        Data Augmentation\n",
    "        \"\"\"\n",
    "        mean = self.x_train.mean().item()\n",
    "        std  = self.x_train.std().item()\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomRotation(2),                           # ±2도 회전\n",
    "            transforms.RandomResizedCrop(self.image_size, scale=(0.8,1.2)),\n",
    "            transforms.Normalize([mean], [std])\n",
    "        ])\n",
    "\n",
    "    def get_patches(self, images):\n",
    "        \"\"\"\n",
    "        Extract patches from images\n",
    "        \"\"\"\n",
    "        return self.model.get_patches(images)\n",
    "\n",
    "    def see_patches(self, num_image: int):\n",
    "        \"\"\"\n",
    "        Visualizes the patches of a test image at the specified index.\n",
    "        \"\"\"\n",
    "        img = self.x_train[num_image]          # (C, H, W)\n",
    "        patches = self.get_patches(img.unsqueeze(0))  # (1, N, patch_dim)\n",
    "\n",
    "        # 원본 이미지\n",
    "        plt.figure()\n",
    "        plt.title(f\"Wafer #{num_image}\")\n",
    "        plt.imshow(img.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Image size: {self.image_size} x {self.image_size}\")\n",
    "        print(f\"Patch size: {self.patch_size} x {self.patch_size}\")\n",
    "        print(f\"Patches per image: {patches.size(1)}\")\n",
    "        print(f\"Elements per patch (dim): {patches.size(2)}\")\n",
    "        print(f\"Num patch (self.num_patches): {self.num_patches}\")\n",
    "\n",
    "        # 패치 그리드\n",
    "        n = int(math.sqrt(patches.size(1)))\n",
    "        fig, axes = plt.subplots(n, n, figsize=(4, 4))\n",
    "        for i in range(patches.size(1)):\n",
    "            patch = patches[0, i].reshape(self.patch_size, self.patch_size).cpu().detach().numpy()\n",
    "            ax = axes[i // n, i % n]\n",
    "            ax.imshow(patch, cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    def train_model(self, batch_size=512, num_epochs=40, save_path=None):\n",
    "        \"\"\"\n",
    "        Train the model with the given parameters and save the best weights.\n",
    "        \"\"\"\n",
    "        # Dataset / DataLoader 준비\n",
    "        dataset = TensorDataset(self.x_train, self.y_train.float())\n",
    "        val_n = int(len(dataset) * 0.1)\n",
    "        train_n = len(dataset) - val_n\n",
    "        train_ds, val_ds = random_split(dataset, [train_n, val_n])\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
    "        test_loader  = DataLoader(TensorDataset(self.x_test, self.y_test.float()),\n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        history = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # --- Train ---\n",
    "            self.model.train()\n",
    "            running_loss = running_acc = 0.0\n",
    "            total = 0\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "                acc = (preds == yb).float().mean().item()\n",
    "\n",
    "                running_loss += loss.item() * xb.size(0)\n",
    "                running_acc  += acc * xb.size(0)\n",
    "                total += xb.size(0)\n",
    "\n",
    "            train_loss = running_loss / total\n",
    "            train_acc  = running_acc  / total\n",
    "\n",
    "            # --- Validation ---\n",
    "            self.model.eval()\n",
    "            val_loss = val_acc = 0.0\n",
    "            total_v = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    logits = self.model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                    preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "                    acc = (preds == yb).float().mean().item()\n",
    "\n",
    "                    val_loss += loss.item() * xb.size(0)\n",
    "                    val_acc  += acc * xb.size(0)\n",
    "                    total_v += xb.size(0)\n",
    "\n",
    "            val_loss /= total_v\n",
    "            val_acc  /= total_v\n",
    "\n",
    "            \n",
    "            history['loss'].append(train_loss)\n",
    "            history['accuracy'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_accuracy'].append(val_acc)\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_wts = self.model.state_dict()\n",
    "                if save_path:\n",
    "                    torch.save(best_wts, save_path)\n",
    "                    with open(save_path + '_history', 'wb') as f:\n",
    "                        pickle.dump(history, f)\n",
    "\n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}  \"\n",
    "                  f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f}  \"\n",
    "                  f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # Test\n",
    "        self.model.load_state_dict(best_wts)\n",
    "        self.model.eval()\n",
    "        test_loss = test_acc = 0.0\n",
    "        total_t = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                logits = self.model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "                acc = (preds == yb).float().mean().item()\n",
    "\n",
    "                test_loss += loss.item() * xb.size(0)\n",
    "                test_acc  += acc * xb.size(0)\n",
    "                total_t   += xb.size(0)\n",
    "\n",
    "        test_acc = test_acc / total_t\n",
    "        print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n",
    "        return history\n",
    "\n",
    "    def load_model(self, path, plot=False):\n",
    "        \"\"\"\n",
    "        Loads the saved weights and optionally plots the training history.\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        if plot:\n",
    "            with open(path + '_history', 'rb') as f:\n",
    "                history = pickle.load(f)\n",
    "            plt.figure()\n",
    "            plt.plot(history['accuracy'], label='Train Acc')\n",
    "            plt.plot(history['val_accuracy'], label='Val Acc')\n",
    "            plt.plot(history['loss'], label='Train Loss')\n",
    "            plt.plot(history['val_loss'], label='Val Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        return self.model\n",
    "\n",
    "    def model_eval(self, x_test=None, y_test=None, batch_size=512):\n",
    "        \"\"\"\n",
    "        Evaluates the trained model and prints loss and accuracy.\n",
    "        \"\"\"\n",
    "        x_t = x_test if x_test is not None else self.x_test\n",
    "        y_t = y_test if y_test is not None else self.y_test\n",
    "        loader = DataLoader(TensorDataset(x_t, y_t.float()), batch_size=batch_size)\n",
    "\n",
    "        self.model.eval()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        total_loss = total_acc = 0.0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in loader:\n",
    "                logits = self.model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "                acc = (preds == yb).float().mean().item()\n",
    "\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "                total_acc  += acc * xb.size(0)\n",
    "                total      += xb.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        avg_acc  = total_acc  / total\n",
    "        print(f\"Eval loss: {avg_loss:.4f}, Eval accuracy: {avg_acc * 100:.2f}%\")\n",
    "        return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf790432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
