{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e94cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a dataBatch (iterable of [image_batch, label_batch]) \n",
    "# into a tuple of two lists: one list of image batches, another of corresponding label batches.\n",
    "def from_DataBatch_to_list(dataBatch):\n",
    "  val_dataset2 = []  # list to collect image batches\n",
    "  val_dataset2_gt = []  # list to collect ground truth batches\n",
    "  i = 0  # index counter for elements within batch (0: images, 1: labels)\n",
    "\n",
    "  # Iterate over each batch in the provided dataBatch\n",
    "  for batch in dataBatch:\n",
    "    # Each batch is expected to be a sequence where index 0 is images and index 1 is labels\n",
    "    for i in range(len(batch)):\n",
    "      if i == 0:\n",
    "        val_dataset2.append(batch[i])  # append image batch\n",
    "      if i == 1:\n",
    "        val_dataset2_gt.append(batch[i])  # append label batch\n",
    "    \n",
    "  # Return a tuple (images_list, labels_list)\n",
    "  return (val_dataset2, val_dataset2_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21359cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def topk_accuracy(model, k, validation_tuple, device=None):\n",
    "    \"\"\"\n",
    "    Compute Top-K accuracy for a PyTorch model over a validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained PyTorch model.\n",
    "        k (int): Number of top predictions to consider for accuracy.\n",
    "        validation_tuple (tuple): A tuple (val_data, val_labels), where\n",
    "            - val_data is a list of input batches (torch.Tensor) of shape [B, C, H, W]\n",
    "            - val_labels is a list of corresponding label batches, either as class indices [B]\n",
    "        device (torch.device, optional): Device to run inference on. If None, uses CUDA if available.\n",
    "\n",
    "    return:\n",
    "        float: Top-K accuracy over the entire validation set.\n",
    "    \"\"\"\n",
    "    # Unpack validation data and labels\n",
    "    val_data, val_labels = validation_tuple\n",
    "\n",
    "    # Select device: use GPU if available and not specified\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Move model to device and set to evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0  # Counter for correctly predicted samples\n",
    "    total = 0    # Counter for total samples\n",
    "\n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches of inputs and labels\n",
    "        for x_batch, y_batch in zip(val_data, val_labels):\n",
    "            # Move batch to the selected device\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Forward pass: obtain raw logits of shape [B, num_classes]\n",
    "            outputs = model(x_batch)\n",
    "\n",
    "            # Get the indices of the top-k predictions for each sample [B, k]\n",
    "            topk_inds = outputs.topk(k, dim=1).indices\n",
    "\n",
    "            # If labels are one-hot encoded, convert to class indices\n",
    "            if y_batch.dim() > 1:\n",
    "                y_true = y_batch.argmax(dim=1)\n",
    "            else:\n",
    "                y_true = y_batch  # Already class indices\n",
    "\n",
    "            # Compare true labels against top-k predictions: [B, k] boolean tensor\n",
    "            matches = topk_inds.eq(y_true.unsqueeze(1))\n",
    "\n",
    "            # Count samples where the true label is among the top-k predictions\n",
    "            correct += matches.any(dim=1).sum().item()\n",
    "            total += x_batch.size(0)\n",
    "\n",
    "    # Return the ratio of correct predictions\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6152e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(model, failure_types, validationTuple, threshold):\n",
    "  precision = []\n",
    "  recall = []\n",
    "\n",
    "  val_dataset = validationTuple[0]\n",
    "  val_dataset_gt = validationTuple[1]\n",
    "\n",
    "  if (len(threshold)!=0):\n",
    "    # Determine the predicted class for each data in the validation-set and build \n",
    "    # the corresponding list containing the ground truth\n",
    "    for i in range(len(val_dataset)): \n",
    "      if i == 0:\n",
    "        prediction = model.predict(val_dataset[i])\n",
    "        prediction_gt = list(map(lambda x: np.asarray(x).argmax(), val_dataset_gt[i]))\n",
    "        # Determine the predicted class\n",
    "        predicted_class = list(map(lambda x: x[:8].argmax(), prediction))\n",
    "        # Store the probability value for the predicted class\n",
    "        predicted_class_probability = list(map(lambda x: x[:8].max(), prediction))\n",
    "\n",
    "      prediction = model.predict(val_dataset[i])\n",
    "      prediction_gt.extend(list(map(lambda x: np.asarray(x).argmax(), val_dataset_gt[i])))\n",
    "      # Determine the predicted class\n",
    "      predicted_class.extend(list(map(lambda x: x[:8].argmax(), prediction)))\n",
    "      # Store the probability value for the predicted class\n",
    "      predicted_class_probability.extend(list(map(lambda x: x[:8].max(), prediction)))\n",
    "\n",
    "    # Calculate the confusion matrix for each threshold\n",
    "    cm = []\n",
    "    for l in range(len(threshold)):\n",
    "      cm.append([[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "            [0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "            [0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "            [0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "            [0,0,0,0,0,0,0,0,0]])\n",
    "\n",
    "      for i in range(len(failure_types)):\n",
    "        for j in range(len(prediction_gt)):\n",
    "          if (prediction_gt[j] == i):\n",
    "            # If the predicted value does not exceed the threshold, Classify the item as a negative class\n",
    "            if (predicted_class_probability[j]<=threshold[l]):\n",
    "                  cm[l][i][8] += 1\n",
    "            else:\n",
    "              for k in range(len(failure_types)):\n",
    "                if (predicted_class[j] == k):\n",
    "                  cm[l][i][k] += 1\n",
    "\n",
    "    return cm\n",
    "\n",
    "  else:\n",
    "    # Determine the predicted class for each data in the validation-set and build \n",
    "    # the corresponding list containing the ground truth\n",
    "    for i in range(len(val_dataset)):\n",
    "      if i == 0:\n",
    "        prediction = model.predict(val_dataset[i])\n",
    "        prediction_gt = list(map(lambda x: np.asarray(x).argmax(), val_dataset_gt[i]))\n",
    "        # Determine the predicted class\n",
    "        predicted_class = list(map(lambda x: x.argmax(), prediction))\n",
    "\n",
    "      prediction = model.predict(val_dataset[i])\n",
    "      prediction_gt.extend(list(map(lambda x: np.asarray(x).argmax(), val_dataset_gt[i])))\n",
    "      # Determine the predicted class\n",
    "      predicted_class.extend(list(map(lambda x: x.argmax(), prediction)))\n",
    "\n",
    "    # Calculate the confusion matrix for each threshold\n",
    "    cm = [[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "              [0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "              [0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "              [0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "              [0,0,0,0,0,0,0,0,0]]\n",
    "\n",
    "    for i in range(len(failure_types)):\n",
    "      for j in range(len(prediction_gt)):\n",
    "        if (prediction_gt[j] == i):\n",
    "          for k in range(len(failure_types)):\n",
    "            if (predicted_class[j] == k):\n",
    "              cm[i][k] += 1\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a596ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_proportion_correctly_classified(c_matrix, failure_types):\n",
    "  dict_ = {}\n",
    "  for i in range(len(failure_types)):\n",
    "    dict_[failure_types[i]] = c_matrix[i][i]/np.sum(c_matrix[i])\n",
    "  \n",
    "  return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9c5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_report(c_matrix):\n",
    "  precision = 0\n",
    "  recall = 0\n",
    "\n",
    "  num = 0\n",
    "  precision_den = 0\n",
    "  recall_den = 0\n",
    "\n",
    "  for i in range(len(c_matrix)):\n",
    "  # Index 8 is assumed to represent the negative class\n",
    "    if (i != 8):\n",
    "      num += c_matrix[i][i]\n",
    "      precision_den += c_matrix[i][i] + c_matrix[8][i]\n",
    "      recall_den += c_matrix[i][i] + c_matrix[i][8]\n",
    "    \n",
    "\n",
    "  precision = num/precision_den\n",
    "  recall = num/recall_den\n",
    "  return (\n",
    "      {'precision': precision, \n",
    "       'recall': recall,\n",
    "       'f1-measure': 2*(precision*recall)/(precision+recall)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9772bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_report(confusion_matrixes):\n",
    "  tp_rates = []\n",
    "  fp_rates = []\n",
    "\n",
    "  for l in range(len(confusion_matrixes)):\n",
    "    tp = 0; fn = 0\n",
    "    fp = 0; tn = 0\n",
    "\n",
    "    cm = confusion_matrixes[l]\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "      if (i != 8):\n",
    "        tp += cm[i][i] \n",
    "        fn += cm[i][8]\n",
    "        fp += cm[8][i]\n",
    "      \n",
    "      if (i == 8):\n",
    "        tn = cm[i][i]\n",
    "  \n",
    "    tp_rates.append(tp/(tp+fn))\n",
    "    fp_rates.append(fp/(fp+tn))\n",
    "\n",
    "  return (tp_rates, fp_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3ffe83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def roc_curve(tp_rates, fp_rates):\n",
    "  tp = copy.deepcopy(tp_rates)\n",
    "  #tp.extend([0])\n",
    "  tp.reverse()\n",
    "\n",
    "  fp = copy.deepcopy(fp_rates)\n",
    "  #fp.extend([0])\n",
    "  fp.reverse()\n",
    "\n",
    "  plt.plot(fp, tp)\n",
    "  plt.plot([0, 1], ls=\"--\")\n",
    "  plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "  plt.xlabel(\"false positive rate\") \n",
    "  plt.ylabel(\"true positive rate\")\n",
    "  plt.title(\"ROC curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c4408c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "threshold1 = np.arange(0, 1, 0.01).tolist()\n",
    "threshold2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64355b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
